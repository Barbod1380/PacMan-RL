{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "416e97da-1052-4f55-912b-5c889de14e4a",
   "metadata": {
    "id": "416e97da-1052-4f55-912b-5c889de14e4a"
   },
   "source": [
    "# üü° Introduction: Q-Learning for Pac-Man  \n",
    "\n",
    "## üìå Overview  \n",
    "Reinforcement Learning (RL) is a fundamental paradigm in artificial intelligence where an agent learns to interact with an environment to maximize cumulative rewards. **Q-Learning**, a model-free off-policy algorithm, is one of the most widely used RL techniques for discrete state-action spaces. This notebook presents a Q-learning implementation for training an autonomous agent to play **Pac-Man** using **Pygame**.  \n",
    "\n",
    "## üéØ Goal  \n",
    "The objective of this project is to develop an **intelligent agent** that can navigate the Pac-Man environment optimally by learning from experience, without prior knowledge of the environment‚Äôs transition dynamics. The agent will iteratively refine its **Q-table**, improving its decision-making to achieve **higher scores and longer survival times**.  \n",
    "\n",
    "## üî¨ Why Q-Learning?  \n",
    "Q-Learning is particularly suitable for this problem due to its ability to:  \n",
    "‚úî Handle **discrete** state-action spaces efficiently.  \n",
    "‚úî Learn from **trial-and-error** interactions without requiring a model of the environment.  \n",
    "‚úî Converge to an **optimal policy** given sufficient exploration and iterations.  \n",
    "\n",
    "## ü§ñ How It Works  \n",
    "Q-learning operates by updating a Q-table, where each entry **Q(s, a)** represents the expected reward of taking action **a** in state **s**. The core update rule follows:  \n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $s$, $a$: Current state and action.  \n",
    "- $s'$, $a'$: Next state and optimal next action.  \n",
    "- $r$: Reward received after taking action $a$.  \n",
    "- $\\alpha$ (learning rate): Controls how much new information overrides the old.  \n",
    "- $\\gamma$ (discount factor): Weighs the importance of future rewards.  \n",
    "\n",
    "The agent learns an optimal policy by iteratively refining **Q(s, a)** through experience, balancing **exploration (Œµ-greedy policy)** and **exploitation (greedy selection)**.  \n",
    "\n",
    "## üèó Structure of This Notebook  \n",
    "This notebook will be structured as follows:  \n",
    "1. **Environment Setup** ‚Äì Defining states, actions, and rewards.  \n",
    "2. **Q-Learning Implementation** ‚Äì Training loop, Q-table updates, and exploration strategy.  \n",
    "3. **Training & Results** ‚Äì Running simulations, analyzing performance, and fine-tuning hyperparameters.  \n",
    "4. **Future Enhancements** ‚Äì Exploring potential improvements such as function approximation (Deep Q-Networks).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb57c89-45cb-404f-9ac3-eccbd6059cb7",
   "metadata": {
    "id": "adb57c89-45cb-404f-9ac3-eccbd6059cb7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit\n",
    "import pygame\n",
    "import time\n",
    "import pygame\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ElRmb8VpHVa",
   "metadata": {
    "id": "2ElRmb8VpHVa"
   },
   "source": [
    "# üèó Maze Generation for Pac-Man  \n",
    "\n",
    "## üìå Overview  \n",
    "To create a **playable game map**, we generate a **connected maze** using **Prim‚Äôs Algorithm** and embed it into a **13√ó18 grid** with solid outer walls.  \n",
    "\n",
    "While not the core focus of our **Reinforcement Learning (RL) task**, this step ensures a structured environment where all paths are accessible, making it suitable for training an RL agent.  \n",
    "\n",
    "---\n",
    "\n",
    "## üé≤ Function: `generate_connected_maze()`  \n",
    "\n",
    "### üîç Purpose  \n",
    "Generates a **9√ó14 maze** using **Prim‚Äôs Algorithm**, ensuring all **dots ('D')** are connected.  \n",
    "\n",
    "### üõ† Process  \n",
    "1. **Initialize Grid** ‚Üí Fill with **walls ('W')**.  \n",
    "2. **Start Carving Paths** ‚Üí Use Prim‚Äôs Algorithm to progressively replace walls with **dots ('D')**.  \n",
    "3. **Ensure Connectivity** ‚Üí No isolated regions exist; Pac-Man can reach all areas.  \n",
    "\n",
    "### üì§ Output  \n",
    "Returns a **NumPy array (9√ó14) with 'W' (walls) and 'D' (dots)**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c2fa1a-a703-4667-b10c-2036b7a6dac5",
   "metadata": {
    "id": "90c2fa1a-a703-4667-b10c-2036b7a6dac5"
   },
   "outputs": [],
   "source": [
    "def generate_connected_maze(inner_rows = 9, inner_cols = 14, start_row = 4, start_col = 7):\n",
    "\n",
    "    \"\"\"\n",
    "    Generates a connected maze using Prim's algorithm in the inner area (9x14).\n",
    "    Walls ('W') and dots ('D') are placed such that all dots are reachable from Pac-Man's starting position.\n",
    "\n",
    "    Args:\n",
    "        inner_rows (int): Rows of the playable area (default: 9).\n",
    "        inner_cols (int): Columns of the playable area (default: 14).\n",
    "        start_row (int): Pac-Man's starting row in the inner grid (default: 4).\n",
    "        start_col (int): Pac-Man's starting column in the inner grid (default: 7).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Inner grid (14x9) with 'W' (walls) and 'D' (dots).\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize inner grid with walls ('W')\n",
    "    inner_grid = np.full((inner_rows, inner_cols), 'W', dtype = '<U1')\n",
    "\n",
    "    # Directions: Up, Down, Left, Right (relative movement)\n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "\n",
    "    # Track walls that can be carved to connect paths\n",
    "    walls = []\n",
    "\n",
    "    # Mark Pac-Man's starting position as a path ('D')\n",
    "    inner_grid[start_row, start_col] = 'D'\n",
    "\n",
    "    # Add neighboring walls of the starting cell to the wall list\n",
    "    for dx, dy in directions:\n",
    "        neighbor_row = start_row + dx\n",
    "        neighbor_col = start_col + dy\n",
    "        if 0 <= neighbor_row < inner_rows and 0 <= neighbor_col < inner_cols:\n",
    "            walls.append((neighbor_row, neighbor_col, start_row, start_col))\n",
    "\n",
    "    # Process walls until none remain\n",
    "    while walls:\n",
    "        # Randomly select a wall to potentially carve\n",
    "        wall_idx = random.randrange(len(walls))\n",
    "        wall_row, wall_col, parent_row, parent_col = walls.pop(wall_idx)\n",
    "\n",
    "\n",
    "        # Calculate the cell \"opposite\" to the parent through the wall, this is how we ensure connectivity (carving a path through the wall)\n",
    "        opp_row = wall_row + (wall_row - parent_row)\n",
    "        opp_col = wall_col + (wall_col - parent_col)\n",
    "\n",
    "        # Check if the opposite cell is within bounds\n",
    "        if 0 <= opp_row < inner_rows and 0 <= opp_col < inner_cols:\n",
    "\n",
    "            # If the opposite cell is a wall, carve a path, carve the current wall and opposite cell into paths\n",
    "            if inner_grid[opp_row, opp_col] == 'W':\n",
    "                inner_grid[wall_row, wall_col] = 'D'\n",
    "                inner_grid[opp_row, opp_col] = 'D'\n",
    "\n",
    "                # Add neighboring walls of the opposite cell to the wall list\n",
    "                for dx, dy in directions:\n",
    "                    new_row = opp_row + dx\n",
    "                    new_col = opp_col + dy\n",
    "                    if 0 <= new_row < inner_rows and 0 <= new_col < inner_cols:\n",
    "                        walls.append((new_row, new_col, opp_row, opp_col))\n",
    "\n",
    "    return inner_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0a6aeb-6ddc-4f39-a741-7db5e666bda6",
   "metadata": {
    "id": "8c0a6aeb-6ddc-4f39-a741-7db5e666bda6"
   },
   "source": [
    "## üó∫ Function: `generate_map()`  \n",
    "\n",
    "### üîç Purpose  \n",
    "Creates the **full game map (13√ó18)** by embedding the generated maze into a **walled structure**.  \n",
    "\n",
    "### üõ† Steps  \n",
    "1. **Initialize a Blank Grid (13√ó18)** ‚Üí Filled with empty spaces.  \n",
    "2. **Set Outer Borders** ‚Üí Walls ('W') define the boundary.  \n",
    "3. **Generate Inner Maze (9√ó14)** ‚Üí Calls `generate_connected_maze()`.  \n",
    "4. **Modify Walls for Loops** ‚Üí Converts ~35% of walls into dots to introduce loops.  \n",
    "5. **Embed the Maze into the Full Grid** ‚Üí Placed at `[2:11, 2:16]`.  \n",
    "\n",
    "### üì§ Output  \n",
    "Returns a **13√ó18 NumPy array** representing the **final game map**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eae53c-bf8b-4799-a598-a9c4e74127e5",
   "metadata": {
    "id": "51eae53c-bf8b-4799-a598-a9c4e74127e5"
   },
   "outputs": [],
   "source": [
    "def generate_map():\n",
    "    \"\"\"\n",
    "    Builds the full 13x18 map with:\n",
    "    - 2 Layer Edges as walls ('W').\n",
    "    - Inner area as a connected maze with dots ('D') and Pac-Man ('P').\n",
    "    \"\"\"\n",
    "    # Initialize full grid with empty spaces\n",
    "    full_grid = np.full((13, 18), ' ', dtype='<U1')\n",
    "\n",
    "    # Set outer edges to walls\n",
    "    full_grid[:2, :] = 'W'\n",
    "    full_grid[:, :2] = 'W'   # Left column\n",
    "    full_grid[-2:, :] = 'W'  # Bottom row\n",
    "    full_grid[:, -2:] = 'W'  # Right column\n",
    "\n",
    "    # Generate the inner maze (14x9 playable area)\n",
    "    inner_maze = generate_connected_maze()\n",
    "\n",
    "    for i in range(inner_maze.shape[0]):\n",
    "        for j in range(inner_maze.shape[1]):\n",
    "            if inner_maze[i][j] == 'W':\n",
    "                rnd = np.random.rand()\n",
    "                if rnd < 0.45:\n",
    "                    inner_maze[i][j] = 'D'\n",
    "\n",
    "    # Place the inner maze into the full grid (rows 1-14, columns 1-9)\n",
    "    full_grid[2:11, 2:16] = inner_maze\n",
    "\n",
    "    return full_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7dd89-fe36-4b21-8bd8-d5c71e7e12d7",
   "metadata": {
    "id": "1fa7dd89-fe36-4b21-8bd8-d5c71e7e12d7"
   },
   "source": [
    "# üî¢ `Conv3` Function: Encoding Pac-Man's State into a Numeric Representation  \n",
    "\n",
    "## üìå Overview  \n",
    "In our **Q-Learning implementation**, we need to efficiently represent the **state** of the environment as a **single numeric value** (index) in the **Q-table**.  \n",
    "\n",
    "The function `Conv3()` **encodes** a given **state representation** (a combination of 'D', 'W', 'E') into a **unique number** using **base-3 conversion**.  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Function: `Conv3(combination)`\n",
    "\n",
    "### üîç Purpose  \n",
    "Maps a **state representation** (12 locations around Pac-Man) into a **single number**, which serves as the **index** for our **Q-table**.  \n",
    "\n",
    "### üì• Input  \n",
    "A list of **12 elements**, where each element is one of:  \n",
    "- **'E'** ‚Üí **Empty space**  \n",
    "- **'D'** ‚Üí **Dot**  \n",
    "- **'W'** ‚Üí **Wall**  \n",
    "\n",
    "These elements define **all locations** Pac-Man can **access within a distance of 2**:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\begin{pmatrix}\n",
    "        - & - & C_1 & - & - \\\\\n",
    "        - & C_2 & C_3 & C_4 & - \\\\\n",
    "        C_5 & C_6 & \\text{Agent} & C_7 & C_8 \\\\\n",
    "        - & C_9 & C_{10} & C_{11} & - \\\\\n",
    "        - & - & C_{12} & - & - \\\\\n",
    "    \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "So, if we consider the agent at the center (as shown in the above), the $C_1$ through $C_{12}$ define the state of the agent.\n",
    "\n",
    "\n",
    "### üì§ Output  \n",
    "A **unique integer** representing the **state**, computed using a **base-3 encoding**.  \n",
    "\n",
    "---\n",
    "\n",
    "## üõ† How It Works  \n",
    "### üî¢ Encoding Process  \n",
    "1. **Convert Symbols to Numeric Values**  \n",
    "   - 'E' ‚Üí **0**  \n",
    "   - 'D' ‚Üí **1**  \n",
    "   - 'W' ‚Üí **2**  \n",
    "   \n",
    "2. **Apply Base-3 Conversion**  \n",
    "   - The **rightmost element** represents **\\(3^0\\)**,  \n",
    "   - The **next** represents **\\(3^1\\)**,  \n",
    "   - And so on, forming a **base-3 number**.  \n",
    "\n",
    "   **Example Calculation:**  \n",
    "   ```\n",
    "   Combination: ['D', 'E', 'W', 'D']\n",
    "   Encoded:     [1, 0, 2, 1]\n",
    "   Decimal Value = (1 √ó 3¬≥) + (0 √ó 3¬≤) + (2 √ó 3¬π) + (1 √ó 3‚Å∞)\n",
    "                = (27) + (0) + (6) + (1)\n",
    "                = 34\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Why Use Base-3 Encoding?  \n",
    "- **Compact Representation**: Reduces a **12-character state** to a **single number**.  \n",
    "- **Fast Lookup**: Q-table indexing becomes efficient.  \n",
    "- **Unique Mapping**: Each combination produces a **distinct** index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c5e484-054b-41f9-8d4c-66e35ea8d92a",
   "metadata": {
    "id": "c3c5e484-054b-41f9-8d4c-66e35ea8d92a"
   },
   "outputs": [],
   "source": [
    "def Conv3( combination ):\n",
    "\n",
    "    \"\"\"\n",
    "    Conv3() function works as following:\n",
    "        Input:\n",
    "            - Combination: Is an array of element, and each element is one of the following:\n",
    "                1. E: empty\n",
    "                2. D: dot\n",
    "                3. W: wall\n",
    "        OutPut:\n",
    "            - The out put of this function is a number which is the hase of the initial combination\n",
    "    \"\"\"\n",
    "\n",
    "    coded_comb = [0 for i in range(len(combination))]\n",
    "\n",
    "    encoder = {'E': 0, 'D': 1, 'W': 2}\n",
    "\n",
    "    for index in range(len(combination)):\n",
    "        coded_comb[index] = encoder[combination[index]]\n",
    "\n",
    "    decimal, power = 0, 0\n",
    "    index = len(coded_comb) - 1\n",
    "\n",
    "    while( index >= 0 ):\n",
    "        decimal += coded_comb[index] * 3**power\n",
    "        index -= 1\n",
    "        power += 1\n",
    "\n",
    "    return(decimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dadc181-e9ef-4720-bbf2-d3d6b7edb137",
   "metadata": {
    "id": "9dadc181-e9ef-4720-bbf2-d3d6b7edb137"
   },
   "source": [
    "## üî¢ Q-Table Definition\n",
    "\n",
    "The **Q-Table** is a **3D matrix** with the shape:\n",
    "\n",
    "$$\n",
    "(3^{12}, 4, 4)\n",
    "$$\n",
    "\n",
    "### üìå Explanation of Dimensions:\n",
    "1. **$3^{12}$ (Number of States)**  \n",
    "   - Each state is defined by **12 neighboring positions** around Pac-Man.  \n",
    "   - Each position can have **3 possible values**:\n",
    "     - `E` (Empty)  \n",
    "     - `D` (Dot)  \n",
    "     - `W` (Wall)  \n",
    "   - Since there are **12 positions**, the total number of states is:\n",
    "\n",
    "     $$\n",
    "     3^{12} = 531441\n",
    "     $$\n",
    "\n",
    "2. **First \"4\" (Previous Action)**  \n",
    "   - Represents the **previous move** Pac-Man took:\n",
    "     - `0`: Up  \n",
    "     - `1`: Down  \n",
    "     - `2`: Left  \n",
    "     - `3`: Right  \n",
    "\n",
    "3. **Second \"4\" (Current Action)**  \n",
    "   - Represents the **current action** Pac-Man is evaluating:\n",
    "     - `0`: Up  \n",
    "     - `1`: Down  \n",
    "     - `2`: Left  \n",
    "     - `3`: Right  \n",
    "\n",
    "### üöÄ Purpose:\n",
    "- This structure **prevents Pac-Man from getting stuck in loops** by tracking both the **previous action** and **current action**.\n",
    "- It helps **reinforce efficient movement strategies** and **avoid unnecessary back-and-forth motions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd70546-1a96-49fb-8486-1ba54efda563",
   "metadata": {
    "id": "3bd70546-1a96-49fb-8486-1ba54efda563"
   },
   "outputs": [],
   "source": [
    "universal_mat = np.zeros((3**12, 4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c659f1cb-26bc-42ab-9e1d-c0fc2fcbbb03",
   "metadata": {
    "id": "c659f1cb-26bc-42ab-9e1d-c0fc2fcbbb03"
   },
   "source": [
    "## üéØ Reward Function\n",
    "\n",
    "The **reward function** guides the **Pac-Man agent's behavior** by assigning rewards based on its movement and interactions with the environment.\n",
    "\n",
    "### üìå Reward Rules:\n",
    "- **Moving into an empty space (`E`)** ‚Üí **Small negative reward** (`-0.5`)  \n",
    "- **Reversing direction (opposite of `prev_action`)** ‚Üí **Larger negative reward** (`-1`)  \n",
    "- **Eating a dot (`D`)** ‚Üí **Positive reward** (`4`)  \n",
    "- **Hitting a wall (`W`)** ‚Üí **Not allowed** (the action is removed)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eafa4f1-600e-41ea-85ba-2d583eac8cd4",
   "metadata": {
    "id": "5eafa4f1-600e-41ea-85ba-2d583eac8cd4"
   },
   "outputs": [],
   "source": [
    "def reward_calc(state, action, prev_action):\n",
    "    \"\"\"\n",
    "    reward_calc() function:\n",
    "        - Determines the reward for a given action in a given state.\n",
    "\n",
    "    Parameters:\n",
    "        - state: A list of size 12 representing the agent's neighborhood.\n",
    "                 Example: ['W', 'W', 'D', 'D', 'E', 'D', 'W', 'E', 'D', 'E', 'D']\n",
    "        - action: The action the agent is about to take ('U', 'D', 'L', 'R').\n",
    "        - prev_action: The agent's previous action (used to prevent backtracking).\n",
    "\n",
    "    Returns:\n",
    "        - Reward value (float).\n",
    "    \"\"\"\n",
    "\n",
    "    # Small penalty for moving into an empty space\n",
    "    empty_penalty = -0.5\n",
    "    if action == 'U' and state[2] == 'E':\n",
    "        return empty_penalty\n",
    "    elif action == 'D' and state[9] == 'E':\n",
    "        return empty_penalty\n",
    "    elif action == 'L' and state[5] == 'E':\n",
    "        return empty_penalty\n",
    "    elif action == 'R' and state[6] == 'E':\n",
    "        return empty_penalty\n",
    "\n",
    "    # Larger penalty for reversing direction\n",
    "    reverse_penalty = -1\n",
    "    if (action == 'R' and prev_action == 'L') or \\\n",
    "       (action == 'L' and prev_action == 'R') or \\\n",
    "       (action == 'U' and prev_action == 'D') or \\\n",
    "       (action == 'D' and prev_action == 'U'):\n",
    "        return reverse_penalty\n",
    "\n",
    "    # Positive reward for eating a dot\n",
    "    return 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f31fca-600c-44be-9df5-896880cf90ab",
   "metadata": {
    "id": "c0f31fca-600c-44be-9df5-896880cf90ab"
   },
   "source": [
    "# üîÑ `update_state()` Function Documentation\n",
    "\n",
    "## Overview\n",
    "The `update_state()` function updates the agent‚Äôs position and state after executing an action in the game environment. It ensures that the agent moves correctly without colliding with walls (`W`), updates the game map accordingly, and extracts a new state representation based on the agent's surroundings.\n",
    "\n",
    "---\n",
    "\n",
    "## Parameters\n",
    "\n",
    "- **agent_loc** (`list[int]`):  \n",
    "  The agent's current location in the game grid.  \n",
    "  *Example*: `[1, 7]` (indicating row 1, column 7).\n",
    "\n",
    "- **action** (`str`):  \n",
    "  The action the agent intends to take.  \n",
    "  *Possible values*:  \n",
    "  - `'U'` for Up  \n",
    "  - `'D'` for Down  \n",
    "  - `'L'` for Left  \n",
    "  - `'R'` for Right\n",
    "\n",
    "- **game_map** (`2d numpy.array`):  \n",
    "  The current game map represented as a 2D list of strings, where:  \n",
    "  - `'E'` represents an empty space,  \n",
    "  - `'D'` represents a dot,  \n",
    "  - `'W'` represents a wall,  \n",
    "  - `'A'` represents the agent.\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. **Validate Movement**  \n",
    "   - The function checks if moving in the specified direction is allowed by ensuring that the adjacent cell in that direction is not a wall (`W`).  \n",
    "   - If the cell is not a wall, the agent's location is updated accordingly.\n",
    "\n",
    "2. **Update Agent Location**  \n",
    "   - A copy of the current agent location is made, and based on the action, the agent's new location (`agent_updated_loc`) is computed.\n",
    "\n",
    "3. **Modify the Game Map**  \n",
    "   - The agent's previous location is set to empty (`'E'`).  \n",
    "   - The new location is updated to mark the agent's presence with `'A'`.\n",
    "\n",
    "4. **Extract the Updated State**  \n",
    "   - A 3√ó3 grid centered around the agent's new position is extracted from the updated game map.  \n",
    "   - Four additional positions are inserted to capture:\n",
    "     - The cell 2 steps above the agent,\n",
    "     - The cell 2 steps to the left,\n",
    "     - The cell 2 steps to the right,\n",
    "     - The cell 2 steps below.\n",
    "   - The agent‚Äôs own cell is then removed from this list, and the final state is converted into a NumPy array (`agent_updated_state`).\n",
    "\n",
    "---\n",
    "\n",
    "## Returns\n",
    "\n",
    "- **agent_updated_loc** (`list[int]`):  \n",
    "  The new location of the agent after performing the action.\n",
    "\n",
    "- **agent_updated_state** (`numpy.array`):  \n",
    "  The updated state representation, capturing the agent's immediate surroundings (a combination of the 3√ó3 grid and additional 4 positions).\n",
    "\n",
    "- **game_map** (`2d numpy.array`):  \n",
    "  The updated game map after the agent's move.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8625f07-438d-4897-b0bf-2e167f7757fa",
   "metadata": {
    "id": "e8625f07-438d-4897-b0bf-2e167f7757fa"
   },
   "outputs": [],
   "source": [
    "def update_state(agent_loc, action, game_map):\n",
    "\n",
    "    \"\"\"\n",
    "    The update_state() function works as following:\n",
    "        Input:\n",
    "            1. agent_loc: represent the location of the agent in the game map\n",
    "                example:\n",
    "                    [1, 7]: means the agent is in the first row and seventh column\n",
    "\n",
    "            2. action: represents the agent action (i.e. Up, Down, Left, Right)\n",
    "            3. game_map: represents the map of the game\n",
    "\n",
    "        Output:\n",
    "            1. agent_updated_loc: the new location of the agent after doing an action.\n",
    "            2. agent_updated_state: the new state of the agent.\n",
    "            3. game_map: the updated version of the game map\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    agent_updated_loc = agent_loc.copy()\n",
    "\n",
    "    if( action == 'U' and game_map[agent_loc[0] - 1][agent_loc[1]] != 'W' ):\n",
    "        agent_updated_loc[0] -= 1\n",
    "\n",
    "    elif( action == 'D' and game_map[agent_loc[0] + 1][agent_loc[1]] != 'W' ):\n",
    "        agent_updated_loc[0] += 1\n",
    "\n",
    "    elif( action == 'L' and game_map[agent_loc[0]][agent_loc[1] - 1] != 'W' ):\n",
    "        agent_updated_loc[1] -= 1\n",
    "\n",
    "    elif( action == 'R' and game_map[agent_loc[0]][agent_loc[1] + 1] != 'W' ):\n",
    "        agent_updated_loc[1] += 1\n",
    "\n",
    "    game_map[agent_loc[0]][agent_loc[1]] = 'E'\n",
    "    game_map[agent_updated_loc[0]][agent_updated_loc[1]] = 'A'\n",
    "\n",
    "    agent_updated_state = []\n",
    "    for row in range(3):\n",
    "        for col in range(3):\n",
    "            agent_updated_state.append(game_map[agent_updated_loc[0] - 1 + row][agent_updated_loc[1] - 1 + col])\n",
    "\n",
    "    agent_updated_state.insert(0, game_map[agent_updated_loc[0] - 2][agent_updated_loc[1]])\n",
    "    agent_updated_state.insert(4, game_map[agent_updated_loc[0]][agent_updated_loc[1] - 2])\n",
    "    agent_updated_state.insert(8, game_map[agent_updated_loc[0]][agent_updated_loc[1] + 2])\n",
    "    agent_updated_state.insert(12, game_map[agent_updated_loc[0] + 2][agent_updated_loc[1]])\n",
    "\n",
    "    del agent_updated_state[6]\n",
    "    agent_updated_state = np.array(agent_updated_state)\n",
    "\n",
    "    return(agent_updated_loc, agent_updated_state, game_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e212e22-f43c-4492-820a-26a64d8314c5",
   "metadata": {
    "id": "9e212e22-f43c-4492-820a-26a64d8314c5"
   },
   "source": [
    "# choose_act() Function Documentation\n",
    "\n",
    "## Overview\n",
    "The `choose_act()` function selects an action for the agent based on the current state's Q-values and an exploration strategy. It first removes invalid actions (e.g., those blocked by walls) from consideration. Then, using a dynamic threshold (similar to an epsilon-greedy approach), it either chooses the action with the highest Q-value among the available ones or randomly selects an action. Finally, it encodes the chosen action into a corresponding direction ('U', 'D', 'L', or 'R').\n",
    "\n",
    "---\n",
    "\n",
    "## Parameters\n",
    "\n",
    "- **possible_acts** (`array`):  \n",
    "  An array representing a row of the Q-table. It contains Q-values for each of the four actions (indexed as 0, 1, 2, 3 for 'U', 'D', 'L', and 'R', respectively) for the current state.\n",
    "\n",
    "- **tot_ep** (`int`):  \n",
    "  The total number of episodes planned for training. This value is used to adjust the exploration threshold dynamically.\n",
    "\n",
    "- **curr_ep** (`int`):  \n",
    "  The current episode number. Used with `tot_ep` to calculate a decaying threshold for exploration.\n",
    "\n",
    "- **threshold_init** (`float`):  \n",
    "  The initial threshold (epsilon) value that determines the probability of selecting a random action. This value decays over time to favor exploitation over exploration.\n",
    "\n",
    "- **state** (`array`):  \n",
    "  The state representation of the agent, an array (of size 12) that includes information about the agent's surrounding cells. Specific indices in the array correspond to cells in particular directions:\n",
    "  - **Index 2**: Cell above (for action 'U').\n",
    "  - **Index 5**: Cell to the left (for action 'L').\n",
    "  - **Index 6**: Cell to the right (for action 'R').\n",
    "  - **Index 9**: Cell below (for action 'D').\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. **Initialize Available Actions:**  \n",
    "   Start with all four possible actions\n",
    "\n",
    "\n",
    "2. **Remove Invalid Actions:**  \n",
    "- If `state[2]` is `'W'`, remove action `0` (Up).\n",
    "- If `state[5]` is `'W'`, remove action `2` (Left).\n",
    "- If `state[6]` is `'W'`, remove action `3` (Right).\n",
    "- If `state[9]` is `'W'`, remove action `1` (Down).\n",
    "\n",
    "3. **Compute Dynamic Threshold:**  \n",
    "Calculate the threshold to balance exploration and exploitation.\n",
    "\n",
    "This value decreases as the training progresses, reducing the probability of taking a random action.\n",
    "\n",
    "4. **Select Action:**  \n",
    "- Generate a random number between 0 and 1.\n",
    "- **If the random number is greater than the threshold:**  \n",
    "  - Sort the indices of `possible_acts` in descending order (i.e., highest Q-value first).\n",
    "  - Iterate over these sorted indices and choose the first action that is in the list of available actions.\n",
    "- **Else:**  \n",
    "  - Choose a random action from the available actions.\n",
    "\n",
    "5. **Encode Action:**  \n",
    "Convert the numerical action (0, 1, 2, or 3) into a direction using:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8423d4d-d5e9-4d06-9c1a-6693b37904a2",
   "metadata": {
    "id": "d8423d4d-d5e9-4d06-9c1a-6693b37904a2"
   },
   "outputs": [],
   "source": [
    "def choose_act(possible_acts, tot_ep, curr_ep, threshold_init, state):\n",
    "    \"\"\"\n",
    "    choose_act() function works as following:\n",
    "        Input:\n",
    "            It takes a single input which is an array corresponds to a row of the Q table.\n",
    "\n",
    "        OutPut:\n",
    "            The output of this function is an action that we are going to do (i.e. Up, Down, Left, Right)\n",
    "\n",
    "    This function returns the action with maximum Q by 0.9 probability, and with 0.1 probability, returns a random action.\n",
    "    \"\"\"\n",
    "\n",
    "    availables = np.array([0, 1, 2, 3])\n",
    "    if(state[2] == 'W'):\n",
    "        availables = np.delete(availables, np.argwhere(np.isin(availables, 0)))\n",
    "    if(state[5] == 'W'):\n",
    "        availables = np.delete(availables, np.argwhere(np.isin(availables, 2)))\n",
    "    if(state[6] == 'W'):\n",
    "        availables = np.delete(availables, np.argwhere(np.isin(availables, 3)))\n",
    "    if(state[9] == 'W'):\n",
    "        availables = np.delete(availables, np.argwhere(np.isin(availables, 1)))\n",
    "\n",
    "    threshold = threshold_init * (tot_ep - curr_ep) / tot_ep\n",
    "    rand_num = np.random.rand()\n",
    "\n",
    "    if(rand_num > threshold):\n",
    "        sorted_acts_idx = np.argsort(possible_acts)[::-1]\n",
    "        for i in sorted_acts_idx:\n",
    "            if(np.isin(availables, i).any()):\n",
    "                action = i\n",
    "                break\n",
    "    else:\n",
    "        action = np.random.randint(1, 5) - 1\n",
    "\n",
    "    action_encoder = {0: 'U', 1: 'D', 2: 'L', 3: 'R'}\n",
    "    return(action_encoder[action])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QpcCTCooawyi",
   "metadata": {
    "id": "QpcCTCooawyi"
   },
   "source": [
    "# init_agent() Function Documentation\n",
    "\n",
    "## Overview\n",
    "The `init_agent()` function initializes the agent at a random position within the given map (`mapp`). The function ensures that the agent does not spawn inside a wall ('W') by continuously generating random coordinates until a valid position is found. This random initialization helps increase exploration by allowing the agent to start in different locations across multiple runs.\n",
    "\n",
    "---\n",
    "\n",
    "## Parameters\n",
    "\n",
    "- **mapp** (`2D array`):  \n",
    "  A matrix representing the environment, where each cell corresponds to a position in the map.  \n",
    "  - `'W'` represents a wall (invalid position for the agent).\n",
    "  - Other values represent free spaces where the agent can be placed.\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. **Randomly Generate Coordinates:**  \n",
    "   - The function selects random `x` and `y` coordinates within predefined bounds:\n",
    "     ```python\n",
    "     x = np.random.randint(2, 11)\n",
    "     y = np.random.randint(2, 15)\n",
    "     ```\n",
    "     These bounds ensure the agent is placed within the main playable area of the map.\n",
    "\n",
    "2. **Check for Validity:**  \n",
    "   - The function verifies that the randomly chosen position is not occupied by a wall (`'W'`).  \n",
    "   - If the position is invalid, it repeats the process until a valid position is found.\n",
    "\n",
    "3. **Place the Agent:**  \n",
    "   - Once a valid position is determined, the agent ('A') is placed at `(x, y)`:\n",
    "     ```python\n",
    "     mapp[x, y] = 'A'\n",
    "     ```\n",
    "\n",
    "4. **Return Values:**  \n",
    "   - The function returns:\n",
    "     - The agent's starting position as a list `[x, y]`.\n",
    "     - The updated map with the agent placed.\n",
    "\n",
    "---\n",
    "\n",
    "## Returns\n",
    "\n",
    "- **position** (`list`):  \n",
    "  The randomly chosen valid starting position of the agent as `[x, y]`.\n",
    "\n",
    "- **mapp** (`2D array`):  \n",
    "  The updated map with the agent ('A') placed at the chosen position.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1mxSFa-bEZ0",
   "metadata": {
    "id": "e1mxSFa-bEZ0"
   },
   "outputs": [],
   "source": [
    "def init_agent(mapp):\n",
    "\n",
    "    while True:\n",
    "        x = np.random.randint(2, 11)\n",
    "        y = np.random.randint(2, 15)\n",
    "\n",
    "        if( mapp[x, y] != 'W' ):\n",
    "            break\n",
    "    mapp[x, y] = 'A'\n",
    "\n",
    "    return [x, y], mapp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1d24c-3d78-4a0c-bf71-b2e10f650b6a",
   "metadata": {
    "id": "e7e1d24c-3d78-4a0c-bf71-b2e10f650b6a"
   },
   "source": [
    "# Reinforcement Learning Training Loop Documentation\n",
    "\n",
    "## Overview  \n",
    "This training loop is designed to train an agent using Q-learning in a grid-based environment with dots and walls. The agent learns to navigate the environment while maximizing rewards.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Process  \n",
    "\n",
    "### **1. Map Initialization**  \n",
    "- The environment consists of different maps (`map_id` varies from 0 to 2).  \n",
    "- The map is generated using `generate_map()`, and a copy (`map_reset`) is stored for resetting after each episode.  \n",
    "- The agent starts with an initial action (`prev_action = 'U'`) and a state index (`prev_stated_index = 0`).  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. Episode Execution**  \n",
    "Each episode runs for **1500 iterations** (`programm_counter` varies from 0 to 1499). The following steps occur:\n",
    "\n",
    "#### **2.1. Reset Game State**  \n",
    "- The `game_map` is reset.  \n",
    "- The number of dots (`dots_number`) in the environment is counted.  \n",
    "- Hyperparameters `alpha` (learning rate) and `gamma` (discount factor) are set.  \n",
    "- The agent is placed at a random valid location using `init_agent()`.  \n",
    "- An `action_counter` tracks the number of moves per episode.\n",
    "\n",
    "#### **2.2. Running the Game Loop**  \n",
    "The game runs until one of the following conditions is met:  \n",
    "1. No dots remain (`dots_number == 0`).  \n",
    "2. The agent exceeds **2000** moves (`action_counter >= 2000`).  \n",
    "\n",
    "In each iteration:  \n",
    "1. **Extract Agent State**  \n",
    "   - The local **3√ó3** grid around the agent is captured.  \n",
    "   - Four additional positions (up, left, right, down) are added.  \n",
    "   - The state is converted to an index using `Conv3(state)`.  \n",
    "\n",
    "2. **Choose an Action**  \n",
    "   - The action is chosen using `choose_act()`, which balances exploration and exploitation using an epsilon-decay strategy.  \n",
    "   - Exploration probability decreases as training progresses.  \n",
    "\n",
    "3. **Calculate Reward**  \n",
    "   - The reward is determined by `reward_calc(state, action, prev_action)`.  \n",
    "   - If the agent collects a dot, `dots_number` is decremented.  \n",
    "\n",
    "4. **Update the Game State**  \n",
    "   - The agent's new location, state, and updated `game_map` are obtained using `update_state(agent_location, action, game_map)`.  \n",
    "   - The new state is converted to an index using `Conv3(updated_state)`.  \n",
    "\n",
    "5. **Update the Q-Table**  \n",
    "   - The Q-value for the chosen action is updated using the Q-learning formula:  \n",
    "    \\begin{align*}\n",
    "        Q(s, a) = (1-\\alpha) Q(s, a) + \\alpha(R + \\gamma \\max Q(s', a'))\n",
    "    \\end{align*}\n",
    "   - The update ensures that the agent gradually improves its action-selection policy.  \n",
    "\n",
    "6. **Update Agent Location & Previous Action**  \n",
    "   - The agent moves to `updated_location`.  \n",
    "   - The `action_counter` increments.  \n",
    "   - `prev_action` is updated for the next iteration.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e1b513-6939-4369-ab3c-48577080475e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4e1b513-6939-4369-ab3c-48577080475e",
    "outputId": "102eff43-78f5-45a2-ee4f-c43c765d5f03"
   },
   "outputs": [],
   "source": [
    "for map_id in range(2):\n",
    "    initial_map = generate_map()\n",
    "    map_reset = initial_map\n",
    "    print(\"MAP:\", map_id)\n",
    "\n",
    "    prev_action = 'U'                                        # <----- SetUp Previous Action\n",
    "    prev_stated_index = 0\n",
    "\n",
    "    for programm_counter in range(1500):\n",
    "\n",
    "        game_map = map_reset.copy()\n",
    "        dots_number = np.sum(game_map == 'D')\n",
    "        alpha = 0.3                                          # <----- Define the hyperparameter alpha and gamma.\n",
    "        gamma = 0.5\n",
    "\n",
    "        agent_location, game_map = init_agent(game_map)      # <----- The agent location is the initial position of the agent\n",
    "        action_counter = 0                                   # <----- Count the number of moves in a single game\n",
    "\n",
    "\n",
    "        # We perform the game unitl one of the following condtions met:\n",
    "            # 1. No dots remains\n",
    "            # 2. The game run longer than 1000 moves\n",
    "        while( dots_number > 0 and action_counter < 2000 ):\n",
    "\n",
    "            state = []\n",
    "            for row in range(3):\n",
    "                for col in range(3):\n",
    "                    state.append(game_map[agent_location[0] - 1 + row][agent_location[1] - 1 + col])\n",
    "\n",
    "            state.insert(0, game_map[agent_location[0] - 2][agent_location[1]])\n",
    "            state.insert(4, game_map[agent_location[0]][agent_location[1] - 2])\n",
    "            state.insert(8, game_map[agent_location[0]][agent_location[1] + 2])\n",
    "            state.insert(12, game_map[agent_location[0] + 2][agent_location[1]])\n",
    "\n",
    "            del state[6]\n",
    "            state = np.array(state)\n",
    "            state_index = Conv3( state )\n",
    "\n",
    "            # Choose an action:\n",
    "            action = choose_act(universal_mat[state_index][prev_stated_index], 1500, programm_counter, max(1-(map_id+6)/10, 0.05), state)\n",
    "\n",
    "            # Calculate the reward:\n",
    "            reward = reward_calc(state, action, prev_action)\n",
    "\n",
    "            if( reward == 4 ):\n",
    "                dots_number -= 1\n",
    "\n",
    "            # Update the Q-table & game map:   update_state(agent_loc, action, game_map)\n",
    "            updated_location, updated_state, game_map = update_state(agent_location, action, game_map)\n",
    "            updated_state_index = Conv3(updated_state)\n",
    "\n",
    "            # Update the Q-table:\n",
    "            action_map = {'U': 0, 'D': 1, 'L': 2, 'R': 3}\n",
    "            action_index = action_map[action]\n",
    "            if(prev_action != 'None'):\n",
    "                prev_action_index = action_map[prev_action]\n",
    "                universal_mat[state_index][prev_action_index][action_index] = ((1-alpha)*universal_mat[state_index][prev_action_index][action_index]) + (alpha*(reward+gamma*np.max(universal_mat[updated_state_index][prev_action_index])))\n",
    "\n",
    "            # Update the location:\n",
    "            agent_location = updated_location\n",
    "\n",
    "            # We made a move so incerase the action counter:\n",
    "            action_counter += 1\n",
    "            prev_action = action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d80abb-4f4b-4da2-b471-5e2a8645953e",
   "metadata": {},
   "source": [
    "# Visualization Functions\n",
    "\n",
    "This section documents the functions used for visualizing the Pac-Man game: `draw_pacman`, `draw_map`, and `grid_to_pixel`.\n",
    "\n",
    "---\n",
    "\n",
    "## `draw_pacman(screen, center, radius, mouth_angle)`\n",
    "\n",
    "**Purpose:**  \n",
    "Draws the Pac-Man character with an animated mouth on the given Pygame screen. The mouth is animated by drawing a wedge (polygon) that simulates an opening and closing effect.\n",
    "\n",
    "**Parameters:**  \n",
    "- **`screen`**: The Pygame surface where Pac-Man will be drawn.\n",
    "- **`center`**: A tuple `(x, y)` representing the center coordinates of Pac-Man.\n",
    "- **`radius`**: An integer for the radius of Pac-Man.\n",
    "- **`mouth_angle`**: A float (in radians) representing the total angle of the mouth opening. This angle is typically oscillated over time to create the animation.\n",
    "\n",
    "**Notes:**  \n",
    "- Pac-Man is drawn as a filled circle in yellow.\n",
    "- A wedge (polygon) is drawn in the background color (BLACK) over the circle to create the mouth effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c817336-bba8-4aa9-a142-c3a2f980b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Constants ---\n",
    "WIDTH, HEIGHT = 40, 40\n",
    "WHITE = (255, 255, 255)\n",
    "WALL_COLOR = (50, 50, 50)        # Dark grey for walls\n",
    "YELLOW = (255, 255, 0)           # Bright yellow for Pac-Man\n",
    "BLACK = (0, 0, 0)              # Black background\n",
    "PELLET_COLOR = (200, 200, 200)  # Light grey for pellets\n",
    "\n",
    "# Mouth animation parameters\n",
    "MAX_MOUTH_ANGLE = 0.6  # Maximum opening angle in radians\n",
    "MOUTH_FREQUENCY = 1    # Frequency of oscillation in Hz\n",
    "\n",
    "# Movement animation parameters\n",
    "ANIM_FRAMES = 10       # Number of frames for moving between cells\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "font = pygame.font.Font(None, 36)\n",
    "\n",
    "\n",
    "\n",
    "# --- Pac-Man Drawing with Animated Mouth ---\n",
    "def draw_pacman(screen, center, radius, mouth_angle):\n",
    "    \"\"\"\n",
    "    Draws Pac-Man with an animated mouth.\n",
    "    \n",
    "    Args:\n",
    "        screen: Pygame surface to draw on.\n",
    "        center: Tuple (x, y) representing the center of Pac-Man.\n",
    "        radius: Radius of Pac-Man.\n",
    "        mouth_angle: Total angle (in radians) of the mouth opening.\n",
    "    \"\"\"\n",
    "    # Draw the full Pac-Man circle\n",
    "    pygame.draw.circle(screen, YELLOW, center, radius)\n",
    "    \n",
    "    half_angle = mouth_angle / 2.0\n",
    "    # Pac-Man faces right (0 radians)\n",
    "    start_angle = half_angle\n",
    "    end_angle = -half_angle\n",
    "\n",
    "    x, y = center\n",
    "    point1 = (x + radius * math.cos(start_angle), y - radius * math.sin(start_angle))\n",
    "    point2 = (x + radius * math.cos(end_angle), y - radius * math.sin(end_angle))\n",
    "    \n",
    "    # Draw a polygon (mouth) in the background color\n",
    "    pygame.draw.polygon(screen, BLACK, [center, point1, point2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3172f4bb-10db-4085-9828-e2c732d99389",
   "metadata": {},
   "source": [
    "## `draw_map(screen, map_data, current_mouth_angle, animated_position=None)`\n",
    "\n",
    "**Purpose:**  \n",
    "Visualizes the game map on the Pygame screen. It draws the environment by iterating through the grid and rendering walls, pellets, and Pac-Man. If an `animated_position` is provided, it draws Pac-Man at that pixel location to allow smooth movement transitions.\n",
    "\n",
    "**Parameters:**  \n",
    "- **`screen`**: The Pygame screen object where the game map is rendered.\n",
    "- **`map_data`**: A 2D NumPy array (or similar structure) representing the full game map.\n",
    "  - The map includes cells like `'W'` (wall), `'D'` (dot/pellet), and `'A'` (agent/Pac-Man).\n",
    "- **`current_mouth_angle`**: The current mouth opening angle (in radians) for animating Pac-Man.\n",
    "- **`animated_position`** (optional): A tuple `(x, y)` representing the pixel coordinates where Pac-Man should be drawn. If not provided, Pac-Man is drawn at the default grid location.\n",
    "\n",
    "**Notes:**  \n",
    "- Only the interior of the map (excluding the first and last rows/columns) is rendered for a cleaner display.\n",
    "- Walls are drawn as dark grey rectangles, pellets as small light grey circles, and Pac-Man with the animated drawing provided by `draw_pacman`.\n",
    "\n",
    "---\n",
    "\n",
    "## `grid_to_pixel(cell_row, cell_col)`\n",
    "\n",
    "**Purpose:**  \n",
    "Converts grid cell coordinates from the game map to pixel coordinates on the Pygame screen for the interior display. This is useful for aligning the drawing of moving objects (like Pac-Man) with the grid.\n",
    "\n",
    "**Parameters:**  \n",
    "- **`cell_row`**: The row index of the cell in the game map.\n",
    "- **`cell_col`**: The column index of the cell in the game map.\n",
    "\n",
    "**Returns:**  \n",
    "- A tuple `(pixel_x, pixel_y)` representing the pixel coordinates of the center of the specified grid cell.\n",
    "\n",
    "**Notes:**  \n",
    "- The function adjusts for the fact that the displayed area excludes the outer edges of the map.\n",
    "- The conversion ensures that objects drawn on the grid align perfectly with the rendered map cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b3c29-6675-4073-8abb-8c89cc820c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Map Drawing Function (with optional animated agent) ---\n",
    "def draw_map(screen, map_data, current_mouth_angle, animated_position=None):\n",
    "    \"\"\"\n",
    "    Visualizes the Pac-Man game using Pygame.\n",
    "    Displays only the interior of the map (excluding first and last rows/columns).\n",
    "    If animated_position is provided, the agent ('A') is drawn at that position instead.\n",
    "    \n",
    "    Args:\n",
    "        screen: Pygame screen object.\n",
    "        map_data: The full game map matrix.\n",
    "        current_mouth_angle: Current mouth opening angle for Pac-Man.\n",
    "        animated_position (tuple): (x, y) pixel coordinates to draw Pac-Man.\n",
    "    \"\"\"\n",
    "    screen.fill(BLACK)\n",
    "    rows, cols = map_data.shape\n",
    "    for y in range(1, rows - 1):\n",
    "        for x in range(1, cols - 1):\n",
    "            cell = map_data[y, x]\n",
    "            # Adjust position: interior drawing offset\n",
    "            rect = pygame.Rect((x - 1) * WIDTH, (y - 1) * HEIGHT, WIDTH, HEIGHT)\n",
    "            \n",
    "            if cell == 'W':\n",
    "                pygame.draw.rect(screen, WALL_COLOR, rect)\n",
    "            elif cell == 'D':\n",
    "                pygame.draw.circle(screen, PELLET_COLOR, rect.center, 5)\n",
    "            elif cell == 'A':\n",
    "                # Skip drawing the agent if an animated position is provided.\n",
    "                if animated_position is None:\n",
    "                    pygame.draw.circle(screen, YELLOW, rect.center, WIDTH // 3)\n",
    "                    \n",
    "    # Draw animated Pac-Man if position is provided.\n",
    "    if animated_position is not None:\n",
    "        draw_pacman(screen, animated_position, WIDTH // 3, current_mouth_angle)\n",
    "\n",
    "\n",
    "\n",
    "# --- Helper: Compute Pixel Center for a given grid cell ---\n",
    "def grid_to_pixel(cell_row, cell_col):\n",
    "    \"\"\"\n",
    "    Converts a grid cell position (row, col) from the full map to pixel coordinates\n",
    "    for the interior display. Interior starts at row=1, col=1 in map_data.\n",
    "    \"\"\"\n",
    "    # Adjusted pixel center for interior display:\n",
    "    pixel_x = (cell_col - 1) * WIDTH + WIDTH // 2\n",
    "    pixel_y = (cell_row - 1) * HEIGHT + HEIGHT // 2\n",
    "    return (pixel_x, pixel_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdbabe3-6899-41aa-bd16-bd6b03fa9281",
   "metadata": {},
   "source": [
    "# `run_game()`\n",
    "The final function is the `run_game()` function where we put all things together and simulate the PacMan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c346f44e-28a6-41f3-b6d9-a67ba6b08c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Game Loop with Smooth Transition Animation ---\n",
    "def run_game():\n",
    "    \"\"\"\n",
    "    Runs the main game loop with enhanced graphics, animated Pac-Man,\n",
    "    and smooth movement transitions.\n",
    "    \"\"\"\n",
    "    # Generate the initial game map and initialize dots count and agent position\n",
    "    game_map = generate_map()  # Assume defined elsewhere\n",
    "    dots_number = np.sum(game_map == 'D')\n",
    "    agent_location = [6, 9]\n",
    "    prev_action = 'U'\n",
    "    action_dictionary = {'U': 0, 'D': 1, 'L': 2, 'R': 3}\n",
    "    \n",
    "    counter = 0\n",
    "    total_reward = 0\n",
    "    agent_last10_ind_history = np.zeros(10, dtype=int)\n",
    "    \n",
    "    # Calculate display size for interior (exclude edges)\n",
    "    interior_rows = game_map.shape[0] - 2\n",
    "    interior_cols = game_map.shape[1] - 2\n",
    "    window_size = (WIDTH * interior_cols, HEIGHT * interior_rows)\n",
    "    screen = pygame.display.set_mode(window_size)\n",
    "    pygame.display.set_caption(\"Pac-Man AI\")\n",
    "    \n",
    "    clock = pygame.time.Clock()\n",
    "    start_time = time.time()  # For mouth animation timing\n",
    "    \n",
    "    running = True\n",
    "    while dots_number > 0 and running:\n",
    "        clock.tick(3)  # 30 FPS for smooth animation\n",
    "        \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "        \n",
    "        # Calculate current mouth angle using sine oscillation\n",
    "        elapsed = time.time() - start_time\n",
    "        current_mouth_angle = MAX_MOUTH_ANGLE * (0.5 + 0.5 * math.sin(2 * math.pi * MOUTH_FREQUENCY * elapsed))\n",
    "        \n",
    "        # Build state representation (same as before)\n",
    "        state = []\n",
    "        for row in range(3):\n",
    "            for col in range(3):\n",
    "                state.append(game_map[agent_location[0] - 1 + row][agent_location[1] - 1 + col])\n",
    "        state.insert(0, game_map[agent_location[0] - 2][agent_location[1]])\n",
    "        state.insert(4, game_map[agent_location[0]][agent_location[1] - 2])\n",
    "        state.insert(8, game_map[agent_location[0]][agent_location[1] + 2])\n",
    "        state.insert(12, game_map[agent_location[0] + 2][agent_location[1]])\n",
    "        del state[6]\n",
    "        state = np.array(state)\n",
    "        state_index = Conv3(state)  # Assume defined elsewhere\n",
    "        agent_last10_ind_history[counter % 10] = state_index\n",
    "        \n",
    "        # Choose an action using the Q-table\n",
    "        action = choose_act(universal_mat[state_index][action_dictionary[prev_action]], 1, 0, 0.2, state)\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = reward_calc(state, action, prev_action)\n",
    "        prev_action = action\n",
    "        total_reward += reward\n",
    "        if reward == 4:\n",
    "            dots_number -= 1\n",
    "        \n",
    "        # Store the current agent location before updating\n",
    "        old_location = agent_location.copy()\n",
    "        # Update game state and agent location\n",
    "        updated_location, updated_state, game_map = update_state(agent_location, action, game_map)\n",
    "        agent_location = updated_location\n",
    "        counter += 1\n",
    "        \n",
    "        # Animate smooth transition from old_location to new agent_location\n",
    "        old_pixel = grid_to_pixel(old_location[0], old_location[1])\n",
    "        new_pixel = grid_to_pixel(agent_location[0], agent_location[1])\n",
    "        \n",
    "        for frame in range(ANIM_FRAMES):\n",
    "            t = frame / float(ANIM_FRAMES)  # interpolation factor [0,1]\n",
    "            interp_x = int(old_pixel[0] + (new_pixel[0] - old_pixel[0]) * t)\n",
    "            interp_y = int(old_pixel[1] + (new_pixel[1] - old_pixel[1]) * t)\n",
    "            animated_position = (interp_x, interp_y)\n",
    "            \n",
    "            # Draw map; pass animated_position so Pac-Man is drawn there\n",
    "            draw_map(screen, game_map, current_mouth_angle, animated_position=animated_position)\n",
    "            score_text = font.render(f\"Score: {total_reward}\", True, WHITE)\n",
    "            screen.blit(score_text, (15, 15))\n",
    "            pygame.display.flip()\n",
    "            clock.tick(30)\n",
    "        \n",
    "        # After animation, update display one final time with static position\n",
    "        draw_map(screen, game_map, current_mouth_angle)\n",
    "        score_text = font.render(f\"Score: {total_reward}\", True, WHITE)\n",
    "        screen.blit(score_text, (15, 15))\n",
    "        pygame.display.flip()\n",
    "    \n",
    "    pygame.quit()\n",
    "\n",
    "# Run the game\n",
    "run_game()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
